{"cells":[{"cell_type":"markdown","metadata":{"id":"41YLLHrUJIFO"},"source":["## **Sección 1: Conectandose a la API de OPENAI**"]},{"cell_type":"markdown","metadata":{"id":"A4urwf8bJIFO"},"source":["Para hacer peticiones a la API de OpenAI y crear un script de Python donde el usuario escriba una pregunta y reciba la respuesta generada por ChatGPT, sigue estos pasos:\n","\n","**1. Obtener tu clave de API de OpenAI:**\n","\n","Antes de comenzar, asegúrate de tener una clave de API válida de OpenAI. Puedes obtener una siguiendo las instrucciones en el sitio web de OpenAI.\n","\n","**2. Instalar el paquete `openai`:**\n","\n","Si aún no lo has hecho, instala el paquete `openai` utilizando el siguiente comando:\n","\n","```bash\n","pip install openai\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qfbyk2buepMP"},"source":["probemos la conexion a la Api desde este Colab"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"j4Y4eieIeB8Y"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openai in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (1.2.4)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from openai) (0.25.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from openai) (1.8.0)\n","Requirement already satisfied: anyio<4,>=3.5.0 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from openai) (3.7.1)\n","Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from openai) (4.8.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from openai) (2.5.0)\n","Requirement already satisfied: tqdm>4 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from openai) (4.66.1)\n","Requirement already satisfied: idna>=2.8 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n","Requirement already satisfied: httpcore in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n","Requirement already satisfied: certifi in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n","Requirement already satisfied: annotated-types>=0.4.0 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.1 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n","\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n","You should consider upgrading via the '/Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["#!pip install openai #Instalamos la libreria\n","#!pip install \"openai<1.0.0\"\n","%pip install  openai"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"48h6xx9RB2sf"},"outputs":[{"name":"stderr","output_type":"stream","text":["9364.21s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"]},{"name":"stdout","output_type":"stream","text":["openai 1.2.4\n"]}],"source":["!openai --version\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"bwvFxB49ezSf"},"outputs":[],"source":["import openai #Importamos para tenerla disponible en memoria"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["My key:  None\n"]}],"source":["import os\n","\n","\n","API_KEY = os.environ.get(\"API_KEY\")\n","\n","\n","print(\"My key: \", API_KEY)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n"]}],"source":["import os\n","\n","print(os.environ.get('API_KEY'))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"h7RnpcO_fTDW"},"outputs":[],"source":["openai.api_key = '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' # cargamos la API_KEY"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"btToaOjkfZL_"},"outputs":[],"source":["respuesta = openai.completions.create( ## creamos una instancia de Consulta de Competion\n","        model=\"text-davinci-003\", # modelo a usar \"gpt-3.5-turbo-instruct\" \"text-davinci-003\"\n","        prompt=\"¿Qué es la IA?\", # consulta o prompt\n","        max_tokens=100 # cantidad maxima de tokens\n","    )"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"brRsofd6f_M9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Completion(id='cmpl-8PEqEdE29tYapayip4fJROOSL7FjW', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\n\\nLa Inteligencia Artificial (IA) es un área de la computación cuya objetivo es desarrollar computadoras y sistemas software que sean capaces de realizar tareas que requieren inteligencia humana. En resumen, se trata de la creación de sistemas informáticos con el objetivo de simular el proceso cognitivo humano para')], created=1701025942, model='text-davinci-003', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=8, total_tokens=108), warning='This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations')\n"]}],"source":["print(respuesta)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-LHgjrRogHOW"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","La Inteligencia Artificial (IA) es un área de la computación cuya objetivo es desarrollar computadoras y sistemas software que sean capaces de realizar tareas que requieren inteligencia humana. En resumen, se trata de la creación de sistemas informáticos con el objetivo de simular el proceso cognitivo humano para\n"]}],"source":["print(respuesta.choices[0].text)"]},{"cell_type":"markdown","metadata":{"id":"9qLsQP7bhEQV"},"source":["####Ejemplo de un prompt mas complejo, para clasificar sentientos:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"40nahwS_hP0k"},"outputs":[],"source":["response = openai.completions.create(\n","  model=\"text-davinci-003\",\n","  prompt=\"Decide si el sentimiento de esta frase es positivo, neutral, o negativo. \\\n","  \\n\\nFrase: \\\"Me entusiasma mucho aprender sobre grandes modelos del lenguaje.\\\" \\\n","  \\\"\\nSentiment:\",\n","  temperature=0,\n","  max_tokens=60,\n","  top_p=1.0,\n","  n = 1,\n","  frequency_penalty=0.5,\n","  presence_penalty=0.0\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"qgR_R5_Mhl7E"},"outputs":[{"data":{"text/plain":["' Positivo'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["response.choices[0].text"]},{"cell_type":"markdown","metadata":{"id":"hUZVxrJEBnjw"},"source":["## **Sección 2: Creando Repositorios Git**\n","\n","### **1. Inicializar un Repositorio Local**\n","\n","1. Abre una terminal o símbolo del sistema.\n","2. Navega al directorio de tu proyecto local usando el comando `cd`.\n","3. Inicializa un nuevo repositorio Git con el siguiente comando:\n","\n","```bash\n","git init\n","```\n","\n","### **2. Crear el Archivo `.gitignore`**\n","\n","El archivo `.gitignore` se utiliza para indicar a Git qué archivos o directorios deben ser ignorados y no incluidos en el repositorio. Crea un archivo llamado `.gitignore` en la raíz de tu proyecto y agrega los nombres de archivos y carpetas que deseas ignorar. Por ejemplo:\n","\n","```plaintext\n","# Archivos y directorios ignorados por Git\n","venv/\n","__pycache__/\n","*.pyc\n","*.log\n","```\n","\n","### **3. Agregar Archivos y Confirmaciones**\n","\n","1. Agrega todos los archivos de tu proyecto al área de preparación de Git:\n","\n","```bash\n","git add .\n","```\n","\n","2. Confirma los cambios con un mensaje descriptivo:\n","\n","```bash\n","git commit -m \"Primer commit: Agrega archivos iniciales\"\n","```\n","\n","### **4. Crear un Repositorio en GitHub**\n","\n","1. Abre tu navegador y accede a GitHub (https://github.com).\n","2. Inicia sesión en tu cuenta (o regístrate si no tienes una).\n","3. Haz clic en el botón \"+\" en la esquina superior derecha y selecciona \"Nuevo repositorio\".\n","\n","### **5. Configurar y Enlazar el Repositorio Remoto**\n","\n","1. Completa los detalles del nuevo repositorio (nombre, descripción, opciones, etc.).\n","2. No selecciones la opción de agregar un archivo `README`, ya que ya tienes uno local.\n","3. Una vez creado el repositorio, aparecerá la página con las instrucciones para enlazar tu repositorio local.\n","\n","### **6. Enlazar el Repositorio Remoto**\n","\n","1. En tu terminal, ejecuta los siguientes comandos, reemplazando `<nombre-de-usuario>` con tu nombre de usuario en GitHub y `<nombre-de-repositorio>` con el nombre del repositorio:\n","\n","```bash\n","git remote add origin https://github.com/<nombre-de-usuario>/<nombre-de-repositorio>.git\n","git branch -M main\n","git push -u origin main\n","```\n","\n","### **7. Compartir y Colaborar**\n","\n","Ahora tu repositorio local está enlazado a GitHub. Puedes compartir la URL del repositorio con otros y colaborar en el desarrollo. Para agregar colaboradores, ve a la pestaña \"Settings\" en tu repositorio de GitHub y selecciona \"Manage access\".\n","\n","¡Listo! Has creado un repositorio local, configurado el archivo `.gitignore` y subido tu proyecto a GitHub para compartirlo con otros y colaborar de manera efectiva.\n","\n","### **8. Crear un Archivo de Requisitos con Freeze**\n","\n","Los archivos de requisitos (`requirements.txt`) son útiles para\n","definir todas las dependencias de tu proyecto. Para Crear, usa:\n","\n","```bash\n","pip freeze > requirements.txt\n","```\n","\n","### **9. Instalar Paquetes desde un Archivo de Requisitos**\n","\n","Para instalar paquetes desde un archivo de requisitos, usa:\n","\n","```bash\n","pip install -r requirements.txt\n","```"]},{"cell_type":"markdown","metadata":{"id":"7maXNgK2a5IB"},"source":["## **Sección 3 : Conectando con Python**\n","\n","**1. Crear el Script de Python:**\n","\n","Crea un archivo Python llamado `chatbot.py` (o cualquier nombre que prefieras) y agrega el siguiente código:\n","\n","```python\n","import openai\n","\n","# Configura tu clave de API de OpenAI\n","openai.api_key = 'TU_CLAVE_DE_API_AQUI'\n","\n","def generar_respuesta(pregunta):\n","    respuesta = openai.completions.create(\n","        model=\"text-davinci-003\", # gpt-3.5-turbo-instruct\n","        prompt=pregunta,\n","        max_tokens=50\n","    )\n","    return respuesta.choices[0].text.strip()\n","\n","def main():\n","    print(\"Bienvenido al Chatbot de OpenAI\")\n","    \n","    while True:\n","        pregunta = input(\"Escribe tu pregunta (o 'salir' para finalizar): \")\n","        \n","        if pregunta.lower() == 'salir':\n","            print(\"¡Hasta luego!\")\n","            break\n","        \n","        respuesta = generar_respuesta(pregunta)\n","        print(\"Respuesta:\", respuesta)\n","\n","if __name__ == \"__main__\":\n","    main()\n","```\n","\n","**2. Ejecutar el Script:**\n","\n","Abre una terminal, navega al directorio donde tengas el archivo `chatbot.py` y ejecútalo usando:\n","\n","```bash\n","python chatbot.py\n","```\n","\n","**3. Interactuar con el Chatbot:**\n","\n","Cuando ejecutes el script, podrás interactuar con el chatbot. Escribe preguntas y el chatbot generará respuestas basadas en el motor de lenguaje seleccionado.\n","\n","Ten en cuenta que este ejemplo utiliza el motor `text-davinci-003`. Puedes experimentar con otros motores disponibles en la API de OpenAI para obtener diferentes estilos de respuestas.\n","\n","Recuerda que la respuesta generada puede variar según la pregunta y el contexto. Puedes ajustar los parámetros como `max_tokens` para controlar la longitud de las respuestas generadas."]},{"cell_type":"markdown","metadata":{"id":"Uk3cQx_aJIFO"},"source":["## **Sección 4 : Protegienndo la API-Key**\n","\n","**1. Usando un archivo `config.py`:**\n","\n","Crea un archivo llamado `config.py` en la misma carpeta que tu script principal y guarda tu API key en él:\n","\n","```python\n","# config.py\n","API_KEY = \"tu_clave_de_api_aqui\"\n","```\n","\n","Luego, en tu script principal, importa la clave desde `config.py`:\n","\n","```python\n","from config import API_KEY\n","\n","# Usar API_KEY en tu código\n","print(\"Mi API key es:\", API_KEY)\n","```\n","\n","Este método protege tu clave al mantenerla en un archivo separado. Sin embargo, ten en cuenta que cualquier persona que tenga acceso a tu código también verá la clave si tiene acceso al archivo `config.py`.\n","\n","**2. Usando una variable de entorno del sistema:**\n","\n","Establece una variable de entorno del sistema con el nombre `API_KEY` y asigna tu clave como valor. Esto varía según el sistema operativo.\n","\n","- En Windows, puedes establecer la variable de entorno desde el símbolo del sistema:\n","\n","  ```bash\n","  setx API_KEY \"tu_clave_de_api_aqui\"\n","  ```\n","\n","- En Linux o macOS, puedes agregar la siguiente línea a tu archivo `.bashrc`, `.bash_profile` o `.zshrc`:\n","\n","  ```bash\n","  export API_KEY=\"tu_clave_de_api_aqui\"\n","  ```\n","\n","Luego, en tu script Python, puedes acceder a la variable de entorno utilizando la biblioteca `os`:\n","\n","```python\n","import os\n","\n","API_KEY = os.environ.get(\"API_KEY\")\n","\n","# Usar API_KEY en tu código\n","print(\"Mi API key es:\", API_KEY)\n","```\n","\n","Este método protege aún más tu clave, ya que no está presente en tu código ni en archivos en el directorio. Sin embargo, debes asegurarte de mantener segura tu variable de entorno en tu sistema.\n","\n","Ambos métodos tienen ventajas y desventajas, así que elige el que mejor se adapte a tus necesidades de seguridad y desarrollo."]},{"cell_type":"markdown","metadata":{"id":"tLSe_kvVbSj8"},"source":["## **Sección 5 : Chat Completion**\n","\n","**1. Que es Chat Completion**\n","\n","Chat Completion es un modelo optimizado para generar texto emulando la conversacion con un asistente en un chat.\n","\n","En una conversación a través de la API de OpenAI, los mensajes son una lista de objetos que contienen el contenido de texto de la conversación entre el usuario y el asistente. Y todos ellos tienen dos partes el **ROLE** y el **CONTENT**.\n","\n","Los mensajes se organizan en una conversación secuencial. La conversación comienza con un mensaje del sistema que establece el contexto y el comportamiento esperado del asistente. Luego, se intercalan mensajes de usuario y asistente para simular una interacción natural. El modelo utiliza estos mensajes para generar respuestas coherentes.\n","\n","**2. Roles y mensajes**\n","\n","En la API de OpenAI para Chat Completions, puedes usar tres roles diferentes en los mensajes: \"system,\" \"user,\" y \"assistant.\" Cada rol tiene un propósito específico en la conversación:\n","\n","1. **System (\"system\"):** Este rol se utiliza para proporcionar instrucciones de alto nivel al modelo y configurar el comportamiento del asistente. Las instrucciones del sistema son útiles para establecer el contexto y definir las características generales del asistente en la conversación. Por ejemplo, puedes usar el rol del sistema para establecer reglas como \"You are a helpful assistant,\" lo que indica al modelo cómo debe comportarse durante la conversación.\n","\n","Ejemplo:\n","```python\n","{\"role\": \"system\", \"content\": \"You are a customer support representative.\"}\n","```\n","\n","2. **User (\"user\"):** Este rol se usa para representar los mensajes del usuario en la conversación. Los mensajes del usuario contienen preguntas, solicitudes o cualquier entrada del usuario que desees proporcionar al asistente. El modelo responderá en función de lo que el usuario especifique en estos mensajes.\n","\n","Ejemplo:\n","```python\n","{\"role\": \"user\", \"content\": \"Dame una receta para hacer pastel de chocolate.\"}\n","```\n","\n","3. **Assistant (\"assistant\"):** El rol del asistente se utiliza para representar las respuestas generadas por el modelo en la conversación. Inicialmente, este rol estará vacío, y el modelo completará su contenido en función de los mensajes del sistema y del usuario anteriores en la conversación.\n","\n","Ejemplo:\n","```python\n","{\"role\": \"assistant\", \"content\": \"Aquí tienes una receta deliciosa para hacer pastel de chocolate:\"}\n","```\n","\n","Puedes utilizar estos roles de manera efectiva para estructurar una conversación y guiar al modelo en su interacción contigo. Al configurar los roles y mensajes adecuadamente, puedes obtener respuestas coherentes y contextualmente relevantes del modelo. Ten en cuenta que la conversación es secuencial, y los mensajes se procesan en el orden en que aparecen en la lista de mensajes."]},{"cell_type":"markdown","metadata":{"id":"BYjB1--Yrr1O"},"source":["VEAMOS UN EJEMPLO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHjfDAoSeNsc"},"outputs":[],"source":["response = openai.chat.completions.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=[\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"Eres un útil asistente experto en traducciones de idiomas.\"\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": \"Traduce el siguiente texto en español al ingles: 'Hola, ¿cómo estás?'\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"'Hi, how are you?'\"\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": \"Ahora como se dice \\\"¡Buenos Días!\\\"\"\n","    }\n","  ],\n","  temperature=1,\n","  max_tokens=256,\n","  top_p=1,\n","  frequency_penalty=0,\n","  presence_penalty=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyWT76Qsejy9"},"outputs":[],"source":["print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqyz-pJDes0V"},"outputs":[],"source":["print(response.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{"id":"BGUl9GLukBCJ"},"source":["##**Comparemos los Modelos Completions vs Chat Completions**\n","\n","| Características             | Chat Completions                  | Completions                     |\n","|-----------------------------|-----------------------------------|---------------------------------|\n","| Conversaciones continuas    | Sí, diseñado para diálogos       | No, diseñado para respuestas únicas  |\n","| Estructura de entrada       | Lista de mensajes (roles y contenido) | Una única frase o fragmento  |\n","| Contexto de conversación   | Se mantiene a lo largo de la conversación | No mantiene un contexto de conversación  |\n","| Diálogos interactivos       | Ideal para diálogos simulados y conversaciones en curso  | Adecuado para generación de texto único  |\n","| Roles y sistema            | Puedes definir roles como \"system,\" \"user,\" y \"assistant\" para guiar la conversación  | Sin roles definidos en la entrada  |\n","| Complejidad de entrada     | Puedes incluir múltiples intercambios, simulando conversaciones naturales | Adecuado para solicitudes simples  |\n","| Respuestas contextualmente relevantes | Respuestas más coherentes basadas en el contexto  | Respuestas generadas a partir de un fragmento único  |\n","| Múltiples intercambios     | Puedes extender la conversación con mensajes adicionales | No admite múltiples intercambios  |\n","| Aplicaciones comunes       | Chatbots, asistencia al cliente, generación de texto conversacional  | Completado de texto, generación de contenido único  |\n","Modelos desponibles | Utiliza el modelo \"gpt-3.5-turbo y \"gpt-4\" | Todos los modelos (ver documentación)"]},{"cell_type":"markdown","metadata":{"id":"WSm4h7y_fLpu"},"source":["## **Sección 6 : Interface web con GRADIO**"]},{"cell_type":"markdown","metadata":{"id":"s8CImrBc6ueC"},"source":["## Introducción a Gradio"]},{"cell_type":"markdown","metadata":{"id":"ExMhF5WHB_IY"},"source":["Gradio ofrece dos API diferentes según el nivel de detalle que se busque:\n","\n","- `gradio.Interface`: API de alto nivel que permite crear demos de ML simplemente proporcionando una lista de entradas y salidas.\n","\n","- `gradio.Blocks`: API de bajo nivel que permite tener un control total sobre los flujos de datos y el diseño de la aplicación. Se pueden crear aplicaciones muy complejas de varios pasos utilizando Blocks (como si fueran bloques de construcción).\n","\n","Comenzaremos utilizando `Interface` y al final mostraremos un ejemplo de `Blocks`."]},{"cell_type":"markdown","metadata":{"id":"BSFxN8lAfwOv"},"source":["## Instalamos y usamos Gradio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dI83H8nCBo4E"},"outputs":[],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ku7ZOoh_B2XS"},"outputs":[],"source":["!pip install gradio\n"]},{"cell_type":"markdown","metadata":{"id":"h5EBhuJDgC7X"},"source":["Ejemplo usando una función para saludar que tiene `text` como input y `text` como output.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HMi9kowCpUV"},"outputs":[],"source":["import gradio as gr\n","\n","def saludo(nombre):\n","    return \"Hola \" + nombre + \", ¿Como estas??? \"\n","\n","\n","demo = gr.Interface(\n","    fn=saludo,\n","    inputs = \"text\",\n","    outputs = \"text\"\n",")\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"wkJIvlmtgkfR"},"source":["La clase de `gr.Interface` es una forma fácil de crear demos que pueden ser desde una calculadora hasta una aplicación para reconocimiento de voz.\n","\n","Se inicializa con tres parámetros necesarios:\n","\n","\n","*   `fn`: la función.\n","\n","*   `inputs`: qué componente(s) usar para los inputs de la función, por ejemplo, \"texto\", \"imagen\" o \"audio\"\n","* `outputs`: qué componente(s) usar para los outputs de la función, por ejemplo, \"texto\", \"imagen\" o \"etiqueta\"\n","\n","\n","Gradio incluye más de 20 componentes diferentes, la mayoría de los cuales se pueden utilizar como inputs y outputs. En la documentación está la [lista completa](https://gradio.app/docs/#components)."]},{"cell_type":"markdown","metadata":{"id":"EEgezutgiZ-4"},"source":["Ejemplo 2:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEsPqC8LCGD1"},"outputs":[],"source":["import gradio as gr\n","\n","def saludo(nombre):\n","    return \"Hola \" + nombre + \", ¿Como estas??? \"\n","\n","\n","demo = gr.Interface(\n","    fn=saludo,\n","    inputs = gr.components.Textbox(lines=10, placeholder=\"Dime tu nombre porfa\"),\n","    outputs = \"text\"\n",")\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"YJaJqYuoFOkW"},"source":[]},{"cell_type":"markdown","metadata":{"id":"LnTOzNIQ_R2-"},"source":["## Blocks"]},{"cell_type":"markdown","metadata":{"id":"u4JSbah8BjOZ"},"source":["Creamos un demo que recibe dos modelos. Puede transcribir una voz y también puede clasificar el sentimiento de un texto."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Rmn5K-zG_S_8"},"outputs":[],"source":["%%capture\n","!pip install gradio\n","!pip install transformers\n","!pip install torch"]},{"cell_type":"markdown","metadata":{},"source":["## Pipelines\n","\n","The pipelines are a great and easy way to use models for inference. \n","These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including:\n","- Named Entity Recognition, \n","- Masked Language Modeling, \n","- Sentiment Analysis, \n","- Feature Extraction and \n","- Question Answering. \n","\n","See the task summary for examples of use."]},{"cell_type":"markdown","metadata":{},"source":["There are two categories of pipeline abstractions to be aware about:\n","\n","- The `pipeline()` which is the most powerful object encapsulating all other pipelines.\n"," - Task-specific pipelines are available for `audio`, `computer vision`, `natural language processing`, and `multimodal tasks`."]},{"cell_type":"markdown","metadata":{},"source":["see pipelines at Hugging Face:\n","\n","https://huggingface.co/docs/transformers/main_classes/pipelines"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"XMsS8NZKCwm3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/manuelvargas/Documents/cursos/AI/GenerativeAIwithLLMs/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","config.json: 100%|██████████| 1.29k/1.29k [00:00<00:00, 150kB/s]\n","pytorch_model.bin: 100%|██████████| 1.26G/1.26G [01:11<00:00, 17.6MB/s]\n","Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53-spanish were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n","- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53-spanish and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","tokenizer_config.json: 100%|██████████| 380/380 [00:00<00:00, 46.4kB/s]\n","vocab.json: 100%|██████████| 370/370 [00:00<00:00, 118kB/s]\n","special_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 27.5kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Ignored unknown kwarg option normalize\n","Ignored unknown kwarg option normalize\n","Ignored unknown kwarg option normalize\n","Ignored unknown kwarg option normalize\n"]},{"name":"stderr","output_type":"stream","text":["preprocessor_config.json: 100%|██████████| 158/158 [00:00<00:00, 42.9kB/s]\n","config.json: 100%|██████████| 925/925 [00:00<00:00, 311kB/s]\n","pytorch_model.bin: 100%|██████████| 435M/435M [00:22<00:00, 19.4MB/s] \n","tokenizer_config.json: 100%|██████████| 384/384 [00:00<00:00, 50.2kB/s]\n","tokenizer.json: 100%|██████████| 1.31M/1.31M [00:00<00:00, 16.8MB/s]\n","special_tokens_map.json: 100%|██████████| 167/167 [00:00<00:00, 69.7kB/s]\n"]}],"source":["import gradio as gr\n","from transformers import pipeline\n","\n","trans = pipeline(\"automatic-speech-recognition\", model = \"facebook/wav2vec2-large-xlsr-53-spanish\")\n","clasificador = pipeline(\"text-classification\", model = \"pysentimiento/robertuito-sentiment-analysis\")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"CFGG0R_CFQkd"},"outputs":[],"source":["def audio_a_text(audio):\n","  text = trans(audio)[\"text\"]\n","  return text\n","\n","def texto_a_sentimiento(text):\n","  return clasificador(text)[0][\"label\"]"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"e2xBXVDAF-5-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7860\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["demo = gr.Blocks()\n","\n","with demo:\n","  gr.Markdown(\"Demo de uso de Bloques e integracion de Modelos\")\n","  audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n","\n","  texto = gr.Textbox()\n","  b1 = gr.Button(\"Transcribir\")\n","  b1.click(audio_a_text, inputs=audio, outputs=texto)\n","\n","  label = gr.Label()\n","  b2 = gr.Button(\"Clasificar\")\n","  b2.click(texto_a_sentimiento, inputs=texto, outputs=label)\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"zqK3a-7ygjIi"},"source":["Hagamos un aplicación de blocks un poco más interesante."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"CHuwCu2m0gmC"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7861\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["demo = gr.Blocks()\n","\n","with demo:\n","  gr.Markdown(\"Este es el segundo demo con Blocks\")\n","  with gr.Tabs():\n","    with gr.TabItem(\"Transcribe audio en español\"):\n","      with gr.Row():\n","        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n","        transcripcion = gr.Textbox()\n","      b1 = gr.Button(\"Transcribir\")\n","\n","    with gr.TabItem(\"Análisis de sentimiento en español\"):\n","      with gr.Row():\n","        texto = gr.Textbox()\n","        label = gr.Label()\n","      b2 = gr.Button(\"Sentimiento\")\n","\n","    b1.click(audio_a_text, inputs = audio, outputs=transcripcion)\n","    b2.click(texto_a_sentimiento, inputs=texto, outputs=label)\n","\n","demo.launch()\n","\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
